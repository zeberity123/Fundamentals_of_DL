{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE34rCaSpKbF"
   },
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWzIU04VpKbG"
   },
   "source": [
    "# 합성곱신경망 (Convolutional Neural Networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB7cf0ZEpKbH"
   },
   "source": [
    "이전 섹션에서 우리는 ASL 이미지를 분류하는 간단한 모델을 설계하고 학습했습니다. 모델은 매우 높은 정확도로 학습(Training) 데이터를 올바르게 분류하였지만, 검증(Validation) 데이터셋에서는 잘 작동하지 않았습니다. 학습하지 않은 데이터를 잘 일반화하지 못하는 현상을 [과적합](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)이라고 부르며, 이번 섹션에서 우리는 이미지를 읽고 분류하는데 자주 사용되는 [합성곱신경망(CNN)](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) 이라고 불리는 모델을 배울 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ynTNNznpKbH"
   },
   "source": [
    "## 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqVnN8N7pKbI"
   },
   "source": [
    "* CNN 모델을 위한 데이터 준비\n",
    "* 세련된 형태의 CNN 모델 생성, 다양한 layer들의 이해\n",
    "* CNN 모델 학습 및 성능 관찰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJGGy-QTpKbI"
   },
   "source": [
    "## 데이터 준비 및 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCgKDXs8pKbI"
   },
   "source": [
    "아래 셀은 이전 lab에서 학습한 데이터 전처리 기술이 포함되어 있습니다. 다음 단계로 가기 이전 실행하신 후 검토하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4klbuNBapKbJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "\n",
    "# Load in our data from CSV files\n",
    "train_df = pd.read_csv(\"data/asl_data/sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(\"data/asl_data/sign_mnist_valid.csv\")\n",
    "\n",
    "# Separate out our target values\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "\n",
    "# Separate out our image vectors\n",
    "x_train = train_df.values\n",
    "x_valid = valid_df.values\n",
    "\n",
    "# Turn our scalar targets into binary categories\n",
    "num_classes = 24\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
    "\n",
    "# Normalize our image data\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdBv0jwopKbJ"
   },
   "source": [
    "## CNN을 위한 이미지 변환(Reshaping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGfshcdypKbJ"
   },
   "source": [
    "마지막 연습문제에서, 데이터셋의 개별 형식은 일렬로 된 784픽셀 형식이었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PS5DXC7-pKbK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 784), (7172, 784))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAS1rpyUpKbK"
   },
   "source": [
    "이러한 형식은 서로 가까운 픽셀에 대한 모든 정보를 가지고 있지 않습니다. 그렇기 때문에, feature를 탐지하는 컨볼루션(convolution)을 적용할 수 없습니다. 데이터 셋을 28x28픽셀 형식으로 변경해 보겠습니다. 이를 통해 컨볼루션에서 인접한 픽셀끼리 그룹을 짓고 중요한 feature들을 탐지할 수 있습니다.\n",
    "\n",
    "모델의 첫 번째 컨볼루션 레이어의 경우 이미지의 높이와 너비뿐만 아니라 [채널](https://www.photoshopessentials.com/essentials/rgb/)의 숫자도 요구된다는 점에 유의해야합니다. 우리의 이미지는 Grayscale이기 때문에 채널은 1의 값을 가져야합니다.\n",
    "\n",
    "즉, 현재의 shape인 (27455, 784)을 (27455, 28, 28, 1)로 변환할 필요가 있습니다. 편의상, 우리는 동일하게 유지하기를 원하는 모든 차원에 대해 [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html#numpy.reshape)을 진행하는 경우 '-1'을 통해 표현할 수 있으므로 다음과 같이 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ffLqTGLjpKbK"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_valid = x_valid.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "o8EtX8jYpKbK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aAJ_RClBpKbL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7172, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GkhLg5hzpKbL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 28, 28, 1), (7172, 28, 28, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3PFmGMPpKbL"
   },
   "source": [
    "## 합성곱 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE9OmHGIpKbL"
   },
   "source": [
    "오늘날, 많은 데이터 과학자는 유사한 프로젝트를 기반으로, 자신만의 프로젝트를 시작합니다. 문제가 완전히 새로운 형태가 아닌 경우, 사람들은 [TensorFlow Hub](https://www.tensorflow.org/hub)나 [NGC Catalog](https://ngc.nvidia.com/catalog/models) 와 같은 온라인 저장소에 게시된 모델과 유사한 모델을 활용할 가능성이 높습니다. 오늘, 우리는 이 문제에 다음과 같은 형태의 모델을 활용할 것입니다.\n",
    "\n",
    "<img src=\"images/cnn.png\" width=180 />\n",
    "\n",
    "우리는 강의에서 다양한 종류의 레이어들을 이미 공부했고, 여기서도 공식문서를 나타내는 링크와 함께 여기 있는 모든 레이어들을 학습할 것입니다. 의문 사항이 생길 경우 공식 문서를 읽거나 [stack overflow](https://stackoverflow.com/)에 문의하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U9mFQrdGpKbM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", \n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO8uulVlpKbM"
   },
   "source": [
    "### [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD1_REHFpKbM"
   },
   "source": [
    "<img src=\"images/conv2d.png\" width=300 />\n",
    "\n",
    "2D 컨볼루션 레이어입니다. 작은 커널은 입력 이미지들을 훑으며, 분류에 중요한 특징들을 파악하게 됩니다. 모델의 초기 컨볼루션은 선과 같은 간단한 특징을 탐지하게 됩니다. 그 이후 컨볼루션은 점점 더 복잡한 특징을 탐지하게 됩니다. 이제 우리의 첫 번째 Conv2D layer를 살펴보겠습니다:\n",
    "```Python\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same'...)\n",
    "```\n",
    "75라는 값은 우리가 학습하게 될 필터(Filter)의 갯수를 의미합니다. (3,3) 은 필터의 크기를 의미합니다. strides는 필터가 이미지를 얼마만큼의 간격으로 이동할 것인지를 나타냅니다. 마지막으로, padding은 입력 이미지와 결과 이미지의 크기를 맞추기 위해서 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEEZ_uOFpKbM"
   },
   "source": [
    "### [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) (배치정규화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqvQWh3YpKbN"
   },
   "source": [
    "입력을 정규화하는 것과 마찬가지로, 배치정규화는 hidden layer들의 값을 scaling하여 학습을 개선합니다. [자세한 내용](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2NOoWnHpKbN"
   },
   "source": [
    "### [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3DNUSRrpKbN"
   },
   "source": [
    "<img src=\"images/maxpool2d.png\" width=300 />\n",
    "Max pooling은 이미지를 가져와서 더 낮은 해상도로 축소합니다. 이렇게 하면 모델이 약간의 변화에 더욱 견고하게 만들 수 있고, 모델의 학습 및 추론을 더욱 빠르게 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_odXYjd3pKbO"
   },
   "source": [
    "### [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbAocGkFpKbO"
   },
   "source": [
    "<img src=\"images/dropout.png\" width=360 />\n",
    "드롭아웃은 과적합(Overfitting)을 방지하는 기술입니다. 드롭아웃은 무작위로 뉴런 중 일부를 선택하고 제외시켜,  뉴런들이 특정 경로에서 forward 혹은 backward progation에 참여하지 않도록 합니다. 이를 통해 네트워크가 한 영역에 의존하여 답을 도출하지 않도록 더욱 견고하고 중복적인 형태의 모델을 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCJ72pSMpKbO"
   },
   "source": [
    "### [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) (평탄화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfzi035zpKbO"
   },
   "source": [
    "Flatten은 다차원 입력을 1차원으로 변환시켜 줍니다. Flatten의 결과는 분류를 위한 마지막 layer로 연결되며, feature vector라고 불리게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrRl-fi7pKbO"
   },
   "source": [
    "### [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUq6i7MNpKbP"
   },
   "source": [
    "이전 모델에서 이미 dense 레이어를 다룬 적이 있습니다. 현재 모델의 첫 dense 레이어 (512 유닛)는 feature vector를 입력으로 받으며, 어떠한 Feature가 분류에 기여하는지를 학습하게 됩니다. 두 번째 dense 레이어 (24 유닛)는 우리의 분류 예측 결과를 위한 마지막 layer로 활용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAiuCEZypKbP"
   },
   "source": [
    "## 모델 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO_FTvQOpKbP"
   },
   "source": [
    "다소 복잡하게 느껴질 수 있지만 걱정할 필요는 없습니다. CNN 모델을 효과적으로 훈련하기 위해 지금 당장 모든 것을 이해하는 것은 중요하지 않습니다. 가장 중요한 것은 이미지로부터 유용한 정보를 추출하는 데 도움이 되고 분류 작업에 사용될 수 있다는 것을 알고 계시면 충분합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k38igKj6pKbP"
   },
   "source": [
    "이제, 방금 만든 모델을 요약할 수 있습니다. 이전 노트북의 모델에 비해 학습가능(Trainable)한  매개 변수가 얼마나 적은지 알아 보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ET-2KpJRpKbQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 75)        750       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 75)        300       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 75)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 50)        33800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 50)        200       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 25)          11275     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 7, 25)          100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 25)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               205312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                12312     \n",
      "=================================================================\n",
      "Total params: 264,049\n",
      "Trainable params: 263,749\n",
      "Non-trainable params: 300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN_gAAaspKbQ"
   },
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y75m7b6_pKbQ"
   },
   "source": [
    "이전의 예에서와 동일한 방식으로 모델을 컴파일할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1mZBQN1cpKbQ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHHysIZGpKbQ"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey7HxDeCpKbQ"
   },
   "source": [
    "매우 다른 모델 구조에도 불구하고, 학습은 거의 비슷한 과정으로 진행됩니다. 아래 셀을 실행하여 20 에포크동안 학습하고, 정확도가 향상되는지 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qBIVsbN4pKbR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "858/858 [==============================] - 5s 6ms/step - loss: 0.3075 - accuracy: 0.9058 - val_loss: 0.4242 - val_accuracy: 0.8747\n",
      "Epoch 2/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.1784 - val_accuracy: 0.9491\n",
      "Epoch 3/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.2660 - val_accuracy: 0.9229\n",
      "Epoch 4/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.1914 - val_accuracy: 0.9632\n",
      "Epoch 5/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.3871 - val_accuracy: 0.9297\n",
      "Epoch 6/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.4277 - val_accuracy: 0.9275\n",
      "Epoch 7/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.2472 - val_accuracy: 0.9527\n",
      "Epoch 8/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.1629 - val_accuracy: 0.9704\n",
      "Epoch 9/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.1930 - val_accuracy: 0.9615\n",
      "Epoch 10/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.3897 - val_accuracy: 0.9472\n",
      "Epoch 11/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.1640 - val_accuracy: 0.9650\n",
      "Epoch 12/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.5878 - val_accuracy: 0.9215\n",
      "Epoch 13/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.3459 - val_accuracy: 0.9656\n",
      "Epoch 14/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.5252 - val_accuracy: 0.9431\n",
      "Epoch 15/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.2344 - val_accuracy: 0.9677\n",
      "Epoch 16/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.1983 - val_accuracy: 0.9718\n",
      "Epoch 17/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2874 - val_accuracy: 0.9548\n",
      "Epoch 18/20\n",
      "858/858 [==============================] - 4s 4ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.4080 - val_accuracy: 0.9536\n",
      "Epoch 19/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 8.7343e-04 - accuracy: 0.9998 - val_loss: 0.4064 - val_accuracy: 0.9575\n",
      "Epoch 20/20\n",
      "858/858 [==============================] - 4s 5ms/step - loss: 6.5063e-04 - accuracy: 0.9999 - val_loss: 0.3319 - val_accuracy: 0.9685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1bdf527550>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, verbose=1, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGjWj026pKbR"
   },
   "source": [
    "\n",
    "## 결과에 대한 논의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IGpW3gOpKbR"
   },
   "source": [
    "이 모델은 상당히 개선된 것 같습니다! 학습 정확도도 매우 높고, 검증 정확도도 향상되었습니다. 이것은 좋은 결과이며, 우리는 단지 모델을 새로운 모델로 바꾸기만 하면 됩니다.\n",
    "\n",
    "추가적으로, 검증 정확도가 수렴하지 않는 것을 확인하셨을 수 있습니다. 이것은 여전히 우리 모델이 완벽히 일반화가 되지 않았다는 것을 의미합니다. 다행히도, 우리가 추가적으로 할 수 있는 것이 남아있습니다. 그것은 다음장에서 다루도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86imsNNzpKbR"
   },
   "source": [
    "\n",
    "## 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uR_R4s5pKbR"
   },
   "source": [
    "이 섹션에서는 CNN을 구현하기 위해 몇 가지 새로운 종류의 레이어를 활용하여, 이전 섹션에서 사용된 단순한 모델보다 더욱 높은 성능을 기록할 수 있었습니다. 준비된 데이터로 모델을 만들고 훈련하는 전반적인 과정에 더욱 익숙해지시기를 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96di9fE8pKbS"
   },
   "source": [
    "## 메모리 지우기\n",
    "\n",
    "넘어가기 전에 다음 셀을 실행하여 GPU 메모리를 지워주시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H4xmCp4tpKbS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lznd3JZlpKbS"
   },
   "source": [
    "## 다음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IJ0n_qVpKbS"
   },
   "source": [
    "마지막 몇 가지 섹션에서는 모델의 생성 및 학습에 초점을 맞추었습니다. 이제 성능을 더욱 향상시키기 위해, 모델이 원래 학습하던 것보다 더 많고 좋은 데이터 제공을 위한 *데이터 증강(Augmentation)*이라 불리는 기법을 학습하도록 하겠습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task1_task_03_asl_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
